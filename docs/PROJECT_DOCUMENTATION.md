# ğŸ“Š Proyecto Data Engineer - ReplicaciÃ³n de Base de Datos

## ğŸ“‹ DescripciÃ³n General

Este proyecto implementa un pipeline ETL completo para replicar datos de ventas desde una base de datos PostgreSQL local hacia una base de datos espejo en la nube, automatizando el proceso para ejecutarse diariamente.

### ğŸ¯ Objetivos
- âœ… Crear y poblar base de datos origen con datos CSV
- âœ… Implementar replicaciÃ³n automÃ¡tica a base espejo
- âœ… Automatizar ejecuciÃ³n diaria del pipeline
- âœ… Documentar arquitectura y procesos

## ğŸ—ï¸ Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CSV Files     â”‚â”€â”€â”€â–¶â”‚  PostgreSQL     â”‚â”€â”€â”€â–¶â”‚  Cloud Mirror   â”‚
â”‚                 â”‚    â”‚  (Source DB)    â”‚    â”‚  (Target DB)    â”‚
â”‚ â€¢ DimDate       â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ DimProduct    â”‚    â”‚ bi_schema       â”‚    â”‚ Replicated      â”‚
â”‚ â€¢ DimSegment    â”‚    â”‚ â€¢ dim_date      â”‚    â”‚ Tables          â”‚
â”‚ â€¢ FactSales     â”‚    â”‚ â€¢ dim_product   â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â€¢ dim_segment   â”‚    â”‚                 â”‚
                       â”‚ â€¢ fact_sales    â”‚    â”‚                 â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  ETL Pipeline   â”‚
                       â”‚                 â”‚
                       â”‚ â€¢ Extraction    â”‚
                       â”‚ â€¢ Validation    â”‚
                       â”‚ â€¢ Loading       â”‚
                       â”‚ â€¢ Monitoring    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Estructura del Proyecto

```
Prueba-Tecnica-Data-Engineer/
â”œâ”€â”€ ğŸ“„ README.md                    # DocumentaciÃ³n principal
â”œâ”€â”€ ğŸ“„ requirements.txt             # Dependencias Python
â”œâ”€â”€ ğŸ“„ docker-compose.yml          # ConfiguraciÃ³n Docker
â”œâ”€â”€ ğŸ“„ .env                        # Variables de entorno
â”‚
â”œâ”€â”€ ğŸ“ data/                       # Datos CSV originales
â”‚   â”œâ”€â”€ DimDate (1).csv
â”‚   â”œâ”€â”€ DimCustomerSegment.csv
â”‚   â”œâ”€â”€ DimProduct.csv
â”‚   â””â”€â”€ FactSales.csv
â”‚
â”œâ”€â”€ ğŸ“ database/                   # Scripts de base de datos
â”‚   â”œâ”€â”€ ğŸ“ setup/
â”‚   â”‚   â”œâ”€â”€ create_database.sql    # CreaciÃ³n de BD principal
â”‚   â”‚   â”œâ”€â”€ initial_setup.sql     # ConfiguraciÃ³n inicial
â”‚   â”‚   â””â”€â”€ create_bi_schema.sql   # Esquema optimizado para BI
â”‚   â”œâ”€â”€ ğŸ“ scripts/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ tables/
â”‚   â”‚   â””â”€â”€ ğŸ“ data/
â”‚   â””â”€â”€ ğŸ“ docs/
â”‚
â”œâ”€â”€ ğŸ“ scripts/                    # Scripts ETL y automatizaciÃ³n
â”‚   â”œâ”€â”€ load_csv_data.py          # Carga inicial de CSV
â”‚   â”œâ”€â”€ etl_pipeline.py           # Pipeline de replicaciÃ³n
â”‚   â””â”€â”€ daily_etl_scheduler.py    # AutomatizaciÃ³n diaria
â”‚
â”œâ”€â”€ ğŸ“ docs/                       # DocumentaciÃ³n
â”‚   â””â”€â”€ PROJECT_DOCUMENTATION.md  # Este documento
â”‚
â”œâ”€â”€ ğŸ“ logs/                       # Logs del sistema
â”œâ”€â”€ ğŸ“ reports/                    # Reportes de ejecuciÃ³n
â””â”€â”€ ğŸ“ cloud_mirror/              # Base de datos espejo
```

## ğŸ—ƒï¸ Esquema de Base de Datos

### Base de Datos Origen: `proyecto_data_engineer`

#### Esquema: `bi_schema`

**Tablas de DimensiÃ³n:**

1. **`dim_date`** - DimensiÃ³n temporal
   ```sql
   - dateid (PK)
   - date
   - year, quarter, quarter_name
   - month, month_name
   - day, weekday, weekday_name
   ```

2. **`dim_customer_segment`** - Segmentos de cliente
   ```sql
   - segment_id (PK)
   - city
   ```

3. **`dim_product`** - Productos
   ```sql
   - product_id (PK)
   - product_type
   ```

**Tabla de Hechos:**

4. **`fact_sales`** - Transacciones de ventas
   ```sql
   - sales_id (PK)
   - date_id (FK â†’ dim_date)
   - product_id (FK â†’ dim_product)
   - segment_id (FK â†’ dim_customer_segment)
   - price_per_unit
   - quantity_sold
   - total_amount (calculado)
   ```

### Vistas AnalÃ­ticas

- **`vw_sales_analysis`**: Vista desnormalizada para anÃ¡lisis
- **`vw_monthly_metrics`**: MÃ©tricas agregadas por mes

## ğŸ”§ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Prerrequisitos

```bash
# Software requerido
- Python 3.8+
- PostgreSQL 12+
- Git
```

### 2. InstalaciÃ³n

```bash
# Clonar repositorio
git clone <repository-url>
cd Prueba-Tecnica-Data-Engineer

# Crear entorno virtual
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/macOS

# Instalar dependencias
pip install -r requirements.txt
```

### 3. ConfiguraciÃ³n de Base de Datos

```bash
# 1. Crear base de datos
psql -U postgres -f database/setup/create_database.sql

# 2. Configurar esquema inicial
psql -U postgres -d proyecto_data_engineer -f database/setup/initial_setup.sql

# 3. Crear esquema BI
psql -U postgres -d proyecto_data_engineer -f database/setup/create_bi_schema.sql
```

### 4. ConfiguraciÃ³n de Variables de Entorno

Editar archivo `.env`:
```env
# Base de datos origen
DB_HOST=localhost
DB_PORT=5432
DB_NAME=proyecto_data_engineer
DB_USER=postgres
DB_PASSWORD=tu_password

# ConfiguraciÃ³n ETL
ETL_SCHEDULE_TIME=02:00
LOG_LEVEL=INFO

# Notificaciones (opcional)
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587
EMAIL_FROM=tu-email@gmail.com
EMAIL_TO=admin@proyecto-data-engineer.com
```

## ğŸš€ EjecuciÃ³n del Sistema

### 1. Carga Inicial de Datos

```bash
# Cargar datos CSV en PostgreSQL
python scripts/load_csv_data.py
```

**Salida esperada:**
```
âœ… Cargados 350 registros en dim_date
âœ… Cargados 20 registros en dim_customer_segment  
âœ… Cargados 25 registros en dim_product
âœ… Cargados 26 registros en fact_sales
ğŸ“Š Revenue total: $50,847.74
```

### 2. EjecuciÃ³n Manual del ETL

```bash
# Ejecutar replicaciÃ³n inmediatamente
python scripts/etl_pipeline.py
```

**Salida esperada:**
```
ğŸš€ === INICIANDO REPLICACIÃ“N COMPLETA ===
âœ… ConexiÃ³n a base de datos origen establecida
âœ… ConexiÃ³n a base de datos destino establecida
âœ… Esquema creado en base de datos destino
ğŸ”„ Iniciando replicaciÃ³n de dim_date
âœ… ExtraÃ­dos 350 registros de dim_date
âœ… Cargados 350 registros en dim_date (0.45s)
...
âœ… === REPLICACIÃ“N COMPLETADA EXITOSAMENTE ===
```

### 3. AutomatizaciÃ³n Diaria

```bash
# Iniciar scheduler (ejecuta a las 2:00 AM diariamente)
python scripts/daily_etl_scheduler.py

# Comandos adicionales
python scripts/daily_etl_scheduler.py run           # Ejecutar inmediatamente
python scripts/daily_etl_scheduler.py status       # Ver estado
python scripts/daily_etl_scheduler.py setup-windows # Configurar tarea Windows
```

## ğŸ“Š Monitoreo y Reportes

### Logs del Sistema

Los logs se guardan en:
- `etl_load.log` - Carga inicial de datos
- `etl_pipeline.log` - Ejecuciones del pipeline
- `scheduler.log` - Actividad del scheduler

### Reportes de EjecuciÃ³n

Cada ejecuciÃ³n genera un reporte JSON en `reports/`:
```json
{
  "timestamp": "2025-01-01T02:00:00",
  "total_tables": 4,
  "successful_tables": 4,
  "failed_tables": 0,
  "total_records_processed": 421,
  "total_execution_time": 2.34,
  "tables_detail": [...]
}
```

### MÃ©tricas de ValidaciÃ³n

El sistema valida automÃ¡ticamente:
- âœ… Integridad referencial
- âœ… Conteo de registros origen vs destino
- âœ… MÃ©tricas de negocio (revenue, ventas)

## ğŸ” Consultas de AnÃ¡lisis

### Consultas BÃ¡sicas

```sql
-- Revenue total por mes
SELECT 
    year, month_name,
    SUM(total_amount) as monthly_revenue
FROM vw_sales_analysis 
GROUP BY year, month_name 
ORDER BY year, month;

-- Top productos por ventas
SELECT 
    product_type,
    SUM(quantity_sold) as total_quantity,
    SUM(total_amount) as total_revenue
FROM vw_sales_analysis
GROUP BY product_type
ORDER BY total_revenue DESC;

-- Ventas por ciudad
SELECT 
    city,
    COUNT(*) as transactions,
    SUM(total_amount) as revenue
FROM vw_sales_analysis
GROUP BY city
ORDER BY revenue DESC;
```

### Dashboard Metrics

```sql
-- KPIs principales
SELECT 
    COUNT(*) as total_transactions,
    SUM(total_amount) as total_revenue,
    AVG(total_amount) as avg_transaction,
    COUNT(DISTINCT product_type) as unique_products,
    COUNT(DISTINCT city) as unique_cities
FROM vw_sales_analysis;
```

## ğŸŒ Acceso a Base de Datos Espejo

### InformaciÃ³n de ConexiÃ³n

**Base de Datos Espejo (SQLite para demo):**
- **UbicaciÃ³n**: `./cloud_mirror/data_warehouse.db`
- **Tipo**: SQLite (en producciÃ³n serÃ­a PostgreSQL/MySQL en la nube)
- **Acceso**: Directo via Python/SQLite tools

### ConexiÃ³n ProgramÃ¡tica

```python
import sqlite3

# Conectar a base espejo
conn = sqlite3.connect('./cloud_mirror/data_warehouse.db')

# Consultar datos
cursor = conn.cursor()
cursor.execute("SELECT COUNT(*) FROM fact_sales")
print(f"Total ventas: {cursor.fetchone()[0]}")

conn.close()
```

### Herramientas Recomendadas

- **DB Browser for SQLite** - GUI para SQLite
- **DBeaver** - Cliente universal de BD
- **Python pandas** - AnÃ¡lisis de datos

## ğŸ”§ Mantenimiento

### Tareas Regulares

1. **Monitoreo de Logs**
   ```bash
   # Ver Ãºltimas ejecuciones
   tail -f etl_pipeline.log
   
   # Verificar errores
   grep "ERROR" *.log
   ```

2. **Limpieza de Archivos**
   ```bash
   # Limpiar logs antiguos (>30 dÃ­as)
   find logs/ -name "*.log" -mtime +30 -delete
   
   # Limpiar reportes antiguos (>90 dÃ­as)  
   find reports/ -name "*.json" -mtime +90 -delete
   ```

3. **Backup de Base Espejo**
   ```bash
   # Backup automÃ¡tico
   cp cloud_mirror/data_warehouse.db backups/warehouse_$(date +%Y%m%d).db
   ```

### SoluciÃ³n de Problemas

**Error de ConexiÃ³n a BD:**
```bash
# Verificar servicio PostgreSQL
pg_ctl status

# Verificar conectividad
psql -h localhost -U postgres -d proyecto_data_engineer
```

**ETL Falla:**
```bash
# Verificar logs
cat etl_pipeline.log | tail -50

# Ejecutar en modo debug
python scripts/etl_pipeline.py --debug
```

## ğŸ“ˆ Mejoras Futuras

### Funcionalidades Pendientes

1. **Escalabilidad**
   - Implementar particionado de tablas
   - Optimizar Ã­ndices para grandes volÃºmenes
   - ParalelizaciÃ³n de procesos ETL

2. **Monitoreo Avanzado**
   - Dashboard web en tiempo real
   - Alertas proactivas por Slack/Teams
   - MÃ©tricas de performance detalladas

3. **Seguridad**
   - EncriptaciÃ³n de datos en trÃ¡nsito
   - AutenticaciÃ³n con certificados
   - AuditorÃ­a de accesos

4. **Cloud Native**
   - MigraciÃ³n a AWS RDS/Azure SQL
   - Uso de servicios managed (AWS Glue, Azure Data Factory)
   - ContainerizaciÃ³n con Kubernetes

## ğŸ‘¥ Contacto y Soporte

**Desarrollador:** Sistema ETL  
**Email:** admin@proyecto-data-engineer.com  
**DocumentaciÃ³n:** Este archivo  
**Repositorio:** [GitHub URL]

---

*DocumentaciÃ³n generada automÃ¡ticamente - Ãšltima actualizaciÃ³n: 2025-01-01*
